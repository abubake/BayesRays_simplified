{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from scipy.interpolate import RegularGridInterpolator\n",
    "from dataset_2 import get_rays\n",
    "from ml_helpers import testing\n",
    "\n",
    "# load NeRF model trained in my coral_nerf code\n",
    "model_pth = torch.load('model_training.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test rays of dataset for each test image and pose\n",
    "\n",
    "test_o, test_d, test_target_px_values = get_rays('fox', mode='test') # getting test rays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing out NeRFStudio API way of representing a grid\n",
    "However it appears that to properly use it would require that my NeRF be trained using the NeRFStudio pipeline API\n",
    "To use NeRFStudio:\n",
    "\n",
    "!pip install nerfstudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "from nerfstudio.field_components.encodings import HashEncoding\n",
    "from nerfstudio.field_components.spatial_distortions import SpatialDistortion # maybe I can simply try to distort a sample?\n",
    "\n",
    "lod: int = 8\n",
    "device = 'cpu'\n",
    "\n",
    "ahessian = torch.zeros(((2**lod)+1)**3) #.to(device)\n",
    "deform_field = HashEncoding(num_levels = 1, \n",
    "                            min_res = 2**lod,\n",
    "                            max_res = 2**lod,\n",
    "                            log2_hashmap_size = lod*3+1, #simple regular grid (hash table size > grid size)\n",
    "                            features_per_level = 3,\n",
    "                            hash_init_scale = 0.,\n",
    "                            implementation = \"torch\",\n",
    "                            interpolation = \"Linear\")\n",
    "deform_field.scalings = torch.tensor([2**lod])\n",
    "\n",
    "x = SpatialDistortion.forward(deform_field, positions=[7,4,3]) # not sure how to actually define deformation values in the deformation field...\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other approach to deformation field: Using meshgrid, with a vector [x,y,z] defined at each point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define grid dimensions\n",
    "M = 128\n",
    "x_ = np.arange(0, M)\n",
    "y_ = np.arange(0, M)\n",
    "z_ = np.arange(0, M)\n",
    "\n",
    "# Create meshgrid\n",
    "X, Y, Z = np.meshgrid(x_, y_, z_, indexing='ij')\n",
    "\n",
    "# initalize the grid with .01's (trying to define with single values for now)\n",
    "values = np.ones_like(X)*0.01\n",
    "interp = RegularGridInterpolator((x_, y_, z_), values) # trilinear interpolation function from scipy for getting values between vertices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximating the covariance matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for approximating the hessian from which the covariance can be found. Currently based on the BayesRays approach, still incomplete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .find_uncertainty(points, offsets, outputs['rgb'], pipeline.model.field.spatial_distortion\n",
    "\n",
    "def find_uncertainty(points, deform_points, rgb, distortion):\n",
    "    '''\n",
    "    Approximates the hessian matrix used to find the covariance of the uncertainty field.\n",
    "\n",
    "    Arguments:\n",
    "        - points\n",
    "        - rgb (pixel values in the output)\n",
    "    Outputs:\n",
    "        - Approximated Hessian matrix\n",
    "    '''\n",
    "    colors = torch.sum(rgb, dim=0) # sum all the colors along the ray\n",
    "\n",
    "    # Get r,g,and b channel from deform points\n",
    "    colors[0].backward(retain_graph=True)\n",
    "    r = deform_points.grad.clone().detach().view(-1,3)\n",
    "    deform_points.grad.zero_()\n",
    "    colors[1].backward(retain_graph=True)\n",
    "    g = deform_points.grad.clone().detach().view(-1,3)\n",
    "    deform_points.grad.zero_()\n",
    "    colors[2].backward()\n",
    "    b = deform_points.grad.clone().detach().view(-1,3)\n",
    "    deform_points.grad.zero_()\n",
    "\n",
    "    # continue...\n",
    "    ...\n",
    "    return ahessian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling my rendering function with perturbed [x,y,z] camera positions for each ray to render approximate color for each pixel\n",
    "\n",
    "This is still in progress, figuring out what modificiations to make to the rendering function to successfully add perturbation values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected np.ndarray (got Tensor)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# render with the perturbations: test function calls the perturbed rendering function\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m img,_,psnr \u001b[38;5;241m=\u001b[39m testing(model\u001b[38;5;241m=\u001b[39mmodel_pth, o\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfrom_numpy(test_o[\u001b[38;5;241m4\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat(), d\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfrom_numpy(test_d[\u001b[38;5;241m4\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat(),\n\u001b[1;32m      4\u001b[0m                 tn\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8.\u001b[39m, tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12.\u001b[39m, nb_bins\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m60\u001b[39m, H\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m400\u001b[39m, W\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m400\u001b[39m,target\u001b[38;5;241m=\u001b[39mtest_target_px_values[\u001b[38;5;241m4\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m400\u001b[39m,\u001b[38;5;241m400\u001b[39m,\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m      5\u001b[0m                 ,interp\u001b[38;5;241m=\u001b[39minterp)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/code/coolMath/ml_helpers.py:123\u001b[0m, in \u001b[0;36mtesting\u001b[0;34m(model, o, d, tn, tf, nb_bins, chunk_size, H, W, target, white_bckgr, interp)\u001b[0m\n\u001b[1;32m    121\u001b[0m img \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m o_batch, d_batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(o,d):\n\u001b[0;32m--> 123\u001b[0m     img_batch \u001b[38;5;241m=\u001b[39m rendering(model\u001b[38;5;241m=\u001b[39mmodel,rays_o\u001b[38;5;241m=\u001b[39mo_batch,rays_d\u001b[38;5;241m=\u001b[39md_batch,tn\u001b[38;5;241m=\u001b[39mtn,tf\u001b[38;5;241m=\u001b[39mtf,interp\u001b[38;5;241m=\u001b[39minterp,\n\u001b[1;32m    124\u001b[0m                           nb_bins\u001b[38;5;241m=\u001b[39mnb_bins,device\u001b[38;5;241m=\u001b[39mo_batch\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    125\u001b[0m     img\u001b[38;5;241m.\u001b[39mappend(img_batch) \u001b[38;5;66;03m# [N,3]\u001b[39;00m\n\u001b[1;32m    127\u001b[0m img \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(img)\n",
      "File \u001b[0;32m~/code/coolMath/rendering.py:27\u001b[0m, in \u001b[0;36mrendering\u001b[0;34m(model, rays_o, rays_d, tn, tf, interp, nb_bins, device, white_bckgr)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrendering\u001b[39m(model, rays_o, rays_d, tn, tf, interp, nb_bins\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m, white_bckgr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     23\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03m    The rendering function estimates the color of a ray. Rather than directly computing the integral,\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03m    we use an approximation.\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m     rays_o \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(rays_o)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[1;32m     28\u001b[0m     rays_d \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(rays_d)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[1;32m     30\u001b[0m     t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinspace(tn,tf,nb_bins)\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;66;03m# bounds over which we will compute the integral/sum for determining the color\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected np.ndarray (got Tensor)"
     ]
    }
   ],
   "source": [
    "# render with the perturbations: test function calls the perturbed rendering function\n",
    "\n",
    "img,_,psnr = testing(model=model_pth, o=torch.from_numpy(test_o[4]).to(device).float(), d=torch.from_numpy(test_d[4]).to(device).float(),\n",
    "                tn=8., tf=12., nb_bins=100, chunk_size=60, H=400, W=400,target=test_target_px_values[4].reshape(400,400,3)\n",
    "                ,interp=interp) # interp added to pass interpolation object into the rendering function"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
